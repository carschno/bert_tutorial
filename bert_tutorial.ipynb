{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16af4f95-41cf-424b-8f47-1a498eb1ab7f",
   "metadata": {},
   "source": [
    "# Bert tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6450f9d1-c6c5-4420-af5a-e07b93e8257f",
   "metadata": {},
   "source": [
    "Erik Tjong Kim Sang, e.tjongkimsang@esciencecenter.nl, Netherlands eScience Center"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99598814-6e21-464e-9201-a2ac536b726b",
   "metadata": {},
   "source": [
    "[Bert](https://en.wikipedia.org/wiki/BERT_(language_model)) is a program for processing natural language text. This Jupyter notebook provides a few examples on how you can run Bert. The notebook consists of explanatory text and code snippets. The code snippets can be identified by the two square brackets and the colon on their left [ ]:. You can run the code snippets by clicking on them and then pressing the Shit and Enter keys at the same time. Alternatively you can click on \"Run\" in the top menu and then select \"Run Selected Cells\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690e36dc-9060-498a-bfd2-0711e6cb0640",
   "metadata": {},
   "source": [
    "## 1. An example of Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e6a5ba-8a33-4679-952d-f42c691b2498",
   "metadata": {},
   "source": [
    "Bert is a type of the [transformers](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) machine learning models, developed by Google in 2017.In order to be able to run it, we need to first load the transformers library. This is a standard library of the programming language Python, which we will use in this Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "927a91e3-87fe-44d6-b833-e65eee8b7aa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375e0390-1048-42bc-8e2d-ee47a57e6bc6",
   "metadata": {},
   "source": [
    "Running Bert results in some warning messages. We can turn these off with the next command. If you want to see the warning messages, you can skip the next command. If you want to see the warning messages after having run the next command then you can turn them on again with the command `transformers.utils.logging.set_verbosity_warning()` . Other available message levels are `info` and `debug`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b15c81dc-6acf-4853-8c27-bd867bf0aa14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformers.utils.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01df2044-7aa5-4565-b65f-f7447cc875e8",
   "metadata": {},
   "source": [
    "Bert has been trained on large volumes of English text. It's task was to predict a word (in Bert terms called *token*) given the surrounding words. This is an impossible task to be executed perfectly. However by training Bert on this task, it learned relations between words: certain words can appear in a context while others can not. This is valuable information when processing language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601b9c38-30e4-464e-a080-40c37d84a769",
   "metadata": {},
   "source": [
    "Transformers can perform different tasks based on different models. To run Bert we first need to specify which task we want to perform and what model we want to use. Bert's training task is called `fill-mask`, which involves guessing one missing word in sentence which has been replaced by the special token `[MASK]`. For the model we choose Bert. The other words in the model name will be explained later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54f978ff-18c8-407d-8050-7e606fe2cb43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_bert_pipeline = transformers.pipeline(task='fill-mask', model='bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208a17df-0097-49f3-9e18-c5e34be27e4e",
   "metadata": {},
   "source": [
    "The command has `transformers.pipeline` creates the pipeline `run_bert`. A pipeline is a sequence of software modules which perform a task. Our pipeline `run_bert` can take a sentence with a masked word as input and generate suggestions for the masked word as output. It contains three software modules: 1. a tokenizer, which converts the sentence to numbers; 2. the Bert model, which only processes numbers; 3. post-processing code for among others converting Bert output to text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a14bd8-88ce-467a-86fd-9f8948960312",
   "metadata": {},
   "source": [
    "Next we can apply the Bert pipeline to a sentence. We store the results in the variable `results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "876b41ea-f856-4ed4-8d05-11c505a6de9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = run_bert_pipeline(\"John travelled to [MASK] to visit the Eiffel Tower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028ad51e-5e16-4af1-950f-3e53c4679d83",
   "metadata": {},
   "source": [
    "The output of Bert can be inspected with the following command: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dd2e3d1-7962-4390-bf0b-7dfe48718ace",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.533401608467102,\n",
       "  'token': 2123,\n",
       "  'token_str': 'Paris',\n",
       "  'sequence': 'John travelled to Paris to visit the Eiffel Tower'},\n",
       " {'score': 0.18619701266288757,\n",
       "  'token': 1498,\n",
       "  'token_str': 'London',\n",
       "  'sequence': 'John travelled to London to visit the Eiffel Tower'},\n",
       " {'score': 0.08536867052316666,\n",
       "  'token': 1699,\n",
       "  'token_str': 'France',\n",
       "  'sequence': 'John travelled to France to visit the Eiffel Tower'},\n",
       " {'score': 0.041358355432748795,\n",
       "  'token': 9062,\n",
       "  'token_str': 'Brussels',\n",
       "  'sequence': 'John travelled to Brussels to visit the Eiffel Tower'},\n",
       " {'score': 0.021977808326482773,\n",
       "  'token': 1980,\n",
       "  'token_str': 'Europe',\n",
       "  'sequence': 'John travelled to Europe to visit the Eiffel Tower'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5ffa4c-91e8-44ca-b97f-811142d73423",
   "metadata": {},
   "source": [
    "Bert has provided five suggestions for the masked word. These have been ranked in the output by their score. The highest scoring suggestion is *Paris* which is a good one. *France* and *Europe* are also good suggestions. However *London* and *Brussels* are incorrect in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c79e65-0484-418a-8248-3934c4976636",
   "metadata": {},
   "source": [
    "As you can see Bert has suggested only locations for the missing words. This shows the strength of the Bert language model. There are many words that do not fit at the position of the missing word, like `the` and `knitting`. Bert has learned about this while reading many texts and it does not suggest such words as the missing word of this sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c91013f-fd14-4d56-a299-dda2ac005e8f",
   "metadata": {},
   "source": [
    "### Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ee83b0-f4b4-436b-a040-9e41b064c10e",
   "metadata": {},
   "source": [
    "Make five alternative sentences with a missing word and process them with Bert by replacing the Eiffel Tower sentence above and rerunning the code. Does Bert generate plausible suggestions for the missing words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56041dbc-23c5-49d6-98e8-bb176e848cb1",
   "metadata": {},
   "source": [
    "## 2. Different variants of Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83715395-30de-4efa-8322-d6b98082f41f",
   "metadata": {},
   "source": [
    "There are many different variants of the Bert language model. Alternatives involve the number of training data, the type of training data and the language of the training data. Here are a few frequent keywords of variants:\n",
    "- **base**: the model is trained on a small data set\n",
    "- **large**: the model is trained on a large data set\n",
    "- **cased**: the training data contains upper case characters and lower case characters\n",
    "- **uncased**: the training data was converted to only lower case characters (useful for processing social media text)\n",
    "- **multilingual**: the training data contains text in different languages\n",
    "\n",
    "The website [huggingface.co](https://huggingface.co) contains a large collection of transformer models among which many Bert models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac54c9a-56bd-4ca2-a5d6-4f9314898227",
   "metadata": {},
   "source": [
    "### Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b586c60-2921-4019-b655-35436b14d2d0",
   "metadata": {},
   "source": [
    "Look for an alternative Bert model on the website [huggingface.co](https://huggingface.co), for example a model for another language than English. Create a pipeline for this model by replacing `bert-based-cased` above with the name of this model. Then create five sentences with a missing word and test the model with your sentences. What are your observations? Does Bert generate plausible suggestions for the missing words? If you chose an alternative English model: how does it compare with `bert-based-cased`? Are the missing word suggestions better? How do the processing times compare with `bert-based-cased`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bedb558-4f7e-4053-8115-1f5ead0ce6b0",
   "metadata": {},
   "source": [
    "## 3. Other Bert tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4316572d-3215-421c-8158-b48eaec40729",
   "metadata": {},
   "source": [
    "In natural language processing we work on other tasks than predicting a missing word in a sentence. Examples of such tasks are part-of-speech tagging (finding the syntactic classes of words) and named entity recognition (finding names in text). A challenge for these tasks is that we need human-made examples. These require time and effort to create and we need many of them to train a model that performs well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c24a6e-a059-42a3-bdd2-94d32e228c42",
   "metadata": {},
   "source": [
    "This is where a large general language model like Bert is of great value. Instead of starting the training process for a natural language processing task from zero, we can start from a Bert model. Then we can benefit from the vocabulary and word relations already present in the Bert model. We do not need many examples of our task because Bert will infer that related words unmentioned in our training data need to be treated the same as the words present in our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb297b8-c9a6-4117-85a1-a321927fe9df",
   "metadata": {},
   "source": [
    "Take for example the task of named entity recognition. If we provide Bert with the sentence *John travelled to Paris to visit the Eiffel Tower* and tell it that *Paris* is a location, it will infer that *London*, *France*, *Brussels*, *Europe* and many other similar words are also locations. Bert can draw this conclusion because internally it represents all words as numeric vectors and after its initial training process all words used in similar contexts will have obtained a similar word vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94c7bf8-6712-4c8c-8712-8395601fad5b",
   "metadata": {},
   "source": [
    "The website [huggingface.co](https://huggingface.co) also contains pretrained Bert models for several natural language tasks. Let's test named entity recognition for English. We can use the same pipeline command as above to load the model but this time we need to specify `ner` as task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b75b627-d066-4f37-a34d-e819dfad70c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_bert_pipeline = transformers.pipeline(task='ner', model='dslim/bert-base-NER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41212aab-edae-4754-96d8-90608cbcd360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = run_bert_pipeline(\"John travelled to Paris to see the Eiffel Tower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83de46d7-ec5f-433a-a47d-f99b39596395",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PER',\n",
       "  'score': 0.99826694,\n",
       "  'index': 1,\n",
       "  'word': 'John',\n",
       "  'start': 0,\n",
       "  'end': 4},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': 0.99977285,\n",
       "  'index': 4,\n",
       "  'word': 'Paris',\n",
       "  'start': 18,\n",
       "  'end': 23},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': 0.98286414,\n",
       "  'index': 8,\n",
       "  'word': 'E',\n",
       "  'start': 35,\n",
       "  'end': 36},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.7802712,\n",
       "  'index': 9,\n",
       "  'word': '##iff',\n",
       "  'start': 36,\n",
       "  'end': 39},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': 0.72092605,\n",
       "  'index': 10,\n",
       "  'word': '##el',\n",
       "  'start': 39,\n",
       "  'end': 41},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9886309,\n",
       "  'index': 11,\n",
       "  'word': 'Tower',\n",
       "  'start': 42,\n",
       "  'end': 47}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b452785-534a-49b3-8eaa-c069e9664e16",
   "metadata": {},
   "source": [
    "The output of Bert consists of a list of the names found with their type and score: *John* (person name) and *Paris* and *Eiffel Tower* (locations). We see that *Eiffel Tower* has been split in four parts. From its initial training, Bert has a fixed number of tokens/words. Whenever it encounters a word in a text that is not in this vocabulary (like *Eiffel* in this sentence), it will split this word in tokens that it does know. This is why *Eiffel* is split in *E*, *iff* and *el*. In order to make clear that a word/token was split, the non-initial parts are preceded by two hashes (##) to make clear that they were attached to the previous token(s) in the input. We need to do extra work to put them together from the Bert output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e390a06-2a60-4e51-ac5a-e1b7775844c6",
   "metadata": {},
   "source": [
    "The labels of the tokens in the `entity` fields show another problem of the Bert output. There are two label types in the output `PER` (person name) and `LOC` (location). The first letter of the label indicates whether it is the first word of the entity (`B`) or if it is not (`I`). *John*, *Paris* and *E* are all correctly labelled as entity-initial. But *el* is also labelled as entity-initial and this is incorrect. This is a typical error of Bert-based entity recognizers. In this case it can be corrected with extra post processing which does not allow entities to start in the middle of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0cff0e-0f81-4345-acc5-449444e32fcd",
   "metadata": {},
   "source": [
    "### Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a37577-85d4-4407-b92a-f72d0d70196a",
   "metadata": {},
   "source": [
    "Look on [huggingface.co](https://huggingface.co) for a Bert natural language processing model for your favourite language. Other task definitions than `fill-mask` and `ner` can be found in the [pipeline documentation](https://huggingface.co/transformers/v3.0.2/main_classes/pipelines.html). Create five example sentences for this task and test the model with your sentences. What are your observations? Does Bert generate correct output? Do you observe any weaknesses?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ba6d66-823c-48ed-8a0f-d67243e9f38f",
   "metadata": {},
   "source": [
    "## 4. Training a Bert model for your own NLP task (aka fine-tuning Bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eb7290-b8b2-4572-8ba9-87846f25d37c",
   "metadata": {},
   "source": [
    "Training a Bert model for your own natural language task goes beyond the scope of this tutorial. But if you are interested in this, talk to us to see what the possibilities are. Or take a look at this [Huggingface tutorial](https://huggingface.co/docs/transformers/training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf3ab7-3896-4631-9c22-7187d77e6686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
